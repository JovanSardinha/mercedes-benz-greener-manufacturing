{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'Tilii: https://kaggle.com/tilii7'\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.manifold import TSNE\n",
    "    import pprint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "DATA_PATH = '/kaggle/dev/mercedes-benz-greener-manufacturing-data/raw_data/'\n",
    "SUBMISSION_PATH = '/kaggle/dev/mercedes-benz-greener-manufacturing-data/submissions'\n",
    "ENSEMBLE_PATH = '/kaggle/dev/mercedes-benz-greener-manufacturing-data/ensemble/jsardinha/'\n",
    "ASSET_PATH = '/kaggle/dev/jovan/mercedes-benz-greener-manufacturing/mercedes-benz-greener-manufacturing/assets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Reading and Processing Data\n",
      "\n",
      " Initial Train Set Matrix Dimensions: 4209 x 376\n",
      "\n",
      " Initial Test Set Matrix Dimensions: 4209 x 376\n",
      "\n",
      " Converting categorical features:\n",
      " Converting X0\n",
      " Converting X1\n",
      " Converting X2\n",
      " Converting X3\n",
      " Converting X4\n",
      " Converting X5\n",
      " Converting X6\n",
      " Converting X8\n",
      "\n",
      " Number of columns before cleaning: 579\n",
      " Column X16 is identical to X2_ap. Removing X2_ap\n",
      " Column X17 is identical to X382. Removing X382\n",
      " Column X23 is identical to X2_f. Removing X2_f\n",
      " Column X26 is identical to X2_b. Removing X2_b\n",
      " Column X28 is identical to X2_n. Removing X2_n\n",
      " Column X30 is identical to X2_ag. Removing X2_ag\n",
      " Column X31 is identical to X35. Removing X35\n",
      " Column X31 is identical to X37. Removing X37\n",
      " Column X32 is identical to X2_a. Removing X2_a\n",
      " Column X33 is identical to X39. Removing X39\n",
      " Column X35 is identical to X37. Removing X37\n",
      " Column X36 is identical to X2_z. Removing X2_z\n",
      " Column X44 is identical to X302. Removing X302\n",
      " Column X48 is identical to X113. Removing X113\n",
      " Column X48 is identical to X134. Removing X134\n",
      " Column X48 is identical to X147. Removing X147\n",
      " Column X48 is identical to X222. Removing X222\n",
      " Column X48 is identical to X2_s. Removing X2_s\n",
      " Column X53 is identical to X102. Removing X102\n",
      " Column X53 is identical to X214. Removing X214\n",
      " Column X53 is identical to X239. Removing X239\n",
      " Column X53 is identical to X2_t. Removing X2_t\n",
      " Column X54 is identical to X76. Removing X76\n",
      " Column X58 is identical to X324. Removing X324\n",
      " Column X59 is identical to X2_au. Removing X2_au\n",
      " Column X60 is identical to X253. Removing X253\n",
      " Column X60 is identical to X385. Removing X385\n",
      " Column X62 is identical to X172. Removing X172\n",
      " Column X62 is identical to X216. Removing X216\n",
      " Column X62 is identical to X2_k. Removing X2_k\n",
      " Column X67 is identical to X213. Removing X213\n",
      " Column X67 is identical to X2_aw. Removing X2_aw\n",
      " Column X71 is identical to X84. Removing X84\n",
      " Column X71 is identical to X244. Removing X244\n",
      " Column X83 is identical to X2_q. Removing X2_q\n",
      " Column X84 is identical to X244. Removing X244\n",
      " Column X86 is identical to X2_h. Removing X2_h\n",
      " Column X92 is identical to X2_p. Removing X2_p\n",
      " Column X97 is identical to X2_d. Removing X2_d\n",
      " Column X102 is identical to X214. Removing X214\n",
      " Column X102 is identical to X239. Removing X239\n",
      " Column X102 is identical to X2_t. Removing X2_t\n",
      " Column X107 is identical to X2_ad. Removing X2_ad\n",
      " Column X112 is identical to X199. Removing X199\n",
      " Column X112 is identical to X2_g. Removing X2_g\n",
      " Column X113 is identical to X134. Removing X134\n",
      " Column X113 is identical to X147. Removing X147\n",
      " Column X113 is identical to X222. Removing X222\n",
      " Column X113 is identical to X2_s. Removing X2_s\n",
      " Column X118 is identical to X119. Removing X119\n",
      " Column X125 is identical to X227. Removing X227\n",
      " Column X125 is identical to X2_ac. Removing X2_ac\n",
      " Column X134 is identical to X147. Removing X147\n",
      " Column X134 is identical to X222. Removing X222\n",
      " Column X134 is identical to X2_s. Removing X2_s\n",
      " Column X138 is identical to X146. Removing X146\n",
      " Column X147 is identical to X222. Removing X222\n",
      " Column X147 is identical to X2_s. Removing X2_s\n",
      " Column X152 is identical to X226. Removing X226\n",
      " Column X152 is identical to X326. Removing X326\n",
      " Column X155 is identical to X360. Removing X360\n",
      " Column X172 is identical to X216. Removing X216\n",
      " Column X172 is identical to X2_k. Removing X2_k\n",
      " Column X184 is identical to X262. Removing X262\n",
      " Column X184 is identical to X2_at. Removing X2_at\n",
      " Column X199 is identical to X2_g. Removing X2_g\n",
      " Column X211 is identical to X2_aq. Removing X2_aq\n",
      " Column X213 is identical to X2_aw. Removing X2_aw\n",
      " Column X214 is identical to X239. Removing X239\n",
      " Column X214 is identical to X2_t. Removing X2_t\n",
      " Column X215 is identical to X2_ai. Removing X2_ai\n",
      " Column X216 is identical to X2_k. Removing X2_k\n",
      " Column X222 is identical to X2_s. Removing X2_s\n",
      " Column X226 is identical to X326. Removing X326\n",
      " Column X227 is identical to X2_ac. Removing X2_ac\n",
      " Column X230 is identical to X254. Removing X254\n",
      " Column X232 is identical to X279. Removing X279\n",
      " Column X239 is identical to X2_t. Removing X2_t\n",
      " Column X240 is identical to X364. Removing X364\n",
      " Column X251 is identical to X2_as. Removing X2_as\n",
      " Column X253 is identical to X385. Removing X385\n",
      " Column X257 is identical to X2_o. Removing X2_o\n",
      " Column X260 is identical to X2_am. Removing X2_am\n",
      " Column X262 is identical to X2_at. Removing X2_at\n",
      " Column X277 is identical to X0_bc. Removing X0_bc\n",
      " Column X290 is identical to X293. Removing X293\n",
      " Column X290 is identical to X330. Removing X330\n",
      " Column X293 is identical to X330. Removing X330\n",
      " Column X295 is identical to X296. Removing X296\n",
      " Column X298 is identical to X299. Removing X299\n",
      "\n",
      " Number of columns after cleaning: 517\n",
      "\n",
      " Final Matrix Dimensions: 8418 x 517\n",
      " Time taken: 0 minutes and 3.75 seconds.\n",
      "\n",
      " Calculating t-SNE embedding:\n",
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 8418\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 8418\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 8418\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 8418\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 8418\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 8418\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 8418\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 8418\n",
      "[t-SNE] Computed conditional probabilities for sample 8418 / 8418\n",
      "[t-SNE] Mean sigma: 1.247069\n",
      "[t-SNE] Iteration 25: error = 1.4710975, gradient norm = 0.0039841\n",
      "[t-SNE] Iteration 50: error = 1.4560602, gradient norm = 0.0036393\n",
      "[t-SNE] Iteration 75: error = 1.3648686, gradient norm = 0.0024294\n",
      "[t-SNE] Iteration 100: error = 1.3346347, gradient norm = 0.0021887\n",
      "[t-SNE] KL divergence after 100 iterations with early exaggeration: 1.334635\n",
      "[t-SNE] Iteration 125: error = 1.2733716, gradient norm = 0.0017893\n",
      "[t-SNE] Iteration 150: error = 1.2515035, gradient norm = 0.0016794\n",
      "[t-SNE] Iteration 175: error = 1.2458454, gradient norm = 0.0016520\n",
      "[t-SNE] Iteration 200: error = 1.2442878, gradient norm = 0.0016448\n",
      "[t-SNE] Iteration 225: error = 1.2438730, gradient norm = 0.0016425\n",
      "[t-SNE] Iteration 250: error = 1.2437005, gradient norm = 0.0016423\n",
      "[t-SNE] Iteration 275: error = 1.2436104, gradient norm = 0.0016467\n",
      "[t-SNE] Iteration 300: error = 1.2436422, gradient norm = 0.0016423\n",
      "[t-SNE] Iteration 325: error = 1.2435771, gradient norm = 0.0016469\n",
      "[t-SNE] Iteration 350: error = 1.2435920, gradient norm = 0.0016468\n",
      "[t-SNE] Iteration 375: error = 1.2435663, gradient norm = 0.0016471\n",
      "[t-SNE] Iteration 400: error = 1.2436271, gradient norm = 0.0016425\n",
      "[t-SNE] Iteration 425: error = 1.2435609, gradient norm = 0.0016471\n",
      "[t-SNE] Iteration 425: error difference 0.000000. Finished.\n",
      "[t-SNE] Error after 425 iterations: 1.334635\n",
      " Time taken: 3 minutes and 43.68 seconds.\n",
      "\n",
      " Running Isolation Forest:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/dev/jovan/mercedes-benz-greener-manufacturing/venv-mercedes/lib/python3.5/site-packages/ipykernel_launcher.py:127: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Found 85 outlier points\n",
      " Time taken: 0 minutes and 7.72 seconds.\n",
      "\n",
      " Running Random Forest Regressor (10-fold):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/dev/jovan/mercedes-benz-greener-manufacturing/venv-mercedes/lib/python3.5/site-packages/ipykernel_launcher.py:134: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Time taken: 0 minutes and 13.14 seconds.\n",
      " Time taken: 0 minutes and 13.14 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/dev/jovan/mercedes-benz-greener-manufacturing/venv-mercedes/lib/python3.5/site-packages/ipykernel_launcher.py:151: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# from https://stackoverflow.com/questions/22354094/pythonic-way-of-detecting-outliers-in-one-dimensional-observation-data\n",
    "def is_outlier(points, thresh=3.5):\n",
    "    '''\n",
    "    Returns a boolean array with True if points are outliers and False\n",
    "    otherwise.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "        points : An numobservations by numdimensions array of observations\n",
    "        thresh : The modified z-score to use as a threshold. Observations with\n",
    "            a modified z-score (based on the median absolute deviation) greater\n",
    "            than this value will be classified as outliers.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        mask : A numobservations-length boolean array.\n",
    "\n",
    "    References:\n",
    "    ----------\n",
    "        Boris Iglewicz and David Hoaglin (1993), 'Volume 16: How to Detect and\n",
    "        Handle Outliers', The ASQC Basic References in Quality Control:\n",
    "        Statistical Techniques, Edward F. Mykytka, Ph.D., Editor.\n",
    "    '''\n",
    "    if len(points.shape) == 1:\n",
    "        points = points[:,None]\n",
    "    median = np.median(points, axis=0)\n",
    "    diff = np.sum((points - median)**2, axis=-1)\n",
    "    diff = np.sqrt(diff)\n",
    "    med_abs_deviation = np.median(diff)\n",
    "\n",
    "    modified_z_score = 0.6745 * diff / med_abs_deviation\n",
    "\n",
    "    return (modified_z_score, (modified_z_score > thresh) )\n",
    "\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        tmin, tsec = divmod((datetime.now() - start_time).total_seconds(), 60)\n",
    "        print(' Time taken: %i minutes and %s seconds.' % (tmin, round(tsec,2)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    RFR = RandomForestRegressor(n_estimators=100)\n",
    "    tsne = TSNE(n_components=2, n_iter_without_progress=50, init='pca', verbose=2, random_state=1001)\n",
    "\n",
    "# Load data set and target values\n",
    "    start_time = timer(None)\n",
    "    print('\\n# Reading and Processing Data')\n",
    "    train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'), dtype={'ID': np.int32, 'y': np.float32})\n",
    "    target = train['y'].values\n",
    "    train_ids = train['ID'].values\n",
    "    train = train.drop(['ID', 'y'], axis=1)\n",
    "    print('\\n Initial Train Set Matrix Dimensions: %d x %d' % (train.shape[0], train.shape[1]))\n",
    "    train_len = len(train)\n",
    "    test = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'), dtype={'ID': np.int32})\n",
    "    test_ids = test['ID'].values\n",
    "    test = test.drop(['ID'], axis=1)\n",
    "    print('\\n Initial Test Set Matrix Dimensions: %d x %d' % (test.shape[0], test.shape[1]))\n",
    "\n",
    "# Sort out numerical and categorical features\n",
    "    all_data = pd.concat((train, test))\n",
    "    numeric_feats = all_data.dtypes[all_data.dtypes != 'object'].index\n",
    "    categorical_feats = all_data.dtypes[all_data.dtypes == 'object'].index\n",
    "\n",
    "    print('\\n Converting categorical features:')\n",
    "    for i, col_name in enumerate(categorical_feats):\n",
    "        print(' Converting %s' % col_name)\n",
    "        temp_df = pd.get_dummies(all_data[col_name])\n",
    "        new_features = temp_df.columns.tolist()\n",
    "        new_features = [col_name + '_' + w for w in new_features]\n",
    "        temp_df.columns = new_features\n",
    "        all_data.drop(col_name, axis=1, inplace=True)\n",
    "        all_data = pd.concat((all_data, temp_df), axis=1)\n",
    "\n",
    "# Remove columns where all data points have the same value\n",
    "    print('\\n Number of columns before cleaning: %d' % len(all_data.columns))\n",
    "    cols = all_data.columns.tolist()\n",
    "    for column in cols:\n",
    "        if len(np.unique(all_data[column])) == 1:\n",
    "            print(' Column %s removed' % str(column))\n",
    "            all_data.drop(column, axis=1, inplace=True)\n",
    "\n",
    "# Remove identical columns where all data points have the same value\n",
    "    cols = all_data.columns.tolist()\n",
    "    remove = []\n",
    "    for i in range(len(cols)-1):\n",
    "        v = all_data[cols[i]].values\n",
    "        for j in range(i+1,len(cols)):\n",
    "            if np.array_equal(v,all_data[cols[j]].values):\n",
    "                remove.append(cols[j])\n",
    "                print(' Column %s is identical to %s. Removing %s' % (str(cols[i]), str(cols[j]), str(cols[j])))\n",
    "\n",
    "    all_data.drop(remove, axis=1, inplace=True)\n",
    "    print('\\n Number of columns after cleaning: %d' % len(all_data.columns))\n",
    "\n",
    "    features = all_data.columns\n",
    "    print('\\n Final Matrix Dimensions: %d x %d' % (all_data.shape[0], all_data.shape[1]))\n",
    "    train_data = pd.DataFrame(all_data[ : train_len].values, columns=features)\n",
    "    test_data = pd.DataFrame(all_data[train_len : ].values, columns=features)\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    test_data.reset_index(drop=True, inplace=True)\n",
    "    timer(start_time)\n",
    "\n",
    "    start_time = timer(None)\n",
    "    print('\\n Calculating t-SNE embedding:')\n",
    "    all_data_tsne = tsne.fit_transform(all_data)\n",
    "    train_data_tsne = pd.DataFrame(all_data_tsne[ : train_len], columns=['tsne_x','tsne_y'])\n",
    "    test_data_tsne = pd.DataFrame(all_data_tsne[train_len : ], columns=['tsne_x','tsne_y'])\n",
    "    train_data_tsne.reset_index(drop=True, inplace=True)\n",
    "    test_data_tsne.reset_index(drop=True, inplace=True)\n",
    "    timer(start_time)\n",
    "\n",
    "#Running isolation forest to remove outliers\n",
    "    start_time = timer(None)\n",
    "    clf = IsolationForest(n_estimators=500, max_samples=1.0, random_state=1001, bootstrap=True, contamination=0.02, verbose=0, n_jobs=-1)\n",
    "    print('\\n Running Isolation Forest:')\n",
    "    clf.fit(train_data.values, target)\n",
    "    isof = clf.predict(train_data.values)\n",
    "    train.insert(0, 'y', target)\n",
    "    train.insert(0, 'ID', train_ids)\n",
    "    train['isof'] = isof\n",
    "    myindex = train['isof'] < 0\n",
    "    train_IF = train.loc[myindex]\n",
    "    train_IF.reset_index(drop=True, inplace=True)\n",
    "    train_IF.drop('isof', axis=1, inplace=True)\n",
    "    train_IF.to_csv('train-isof-outliers.csv', index=False)\n",
    "    test.insert(0, 'ID', test_ids)\n",
    "    test['isof'] = clf.predict(test_data.values)\n",
    "    myindex = test['isof'] < 0\n",
    "    test_IF = test.loc[myindex]\n",
    "    test_IF.reset_index(drop=True, inplace=True)\n",
    "    test_IF.drop('isof', axis=1, inplace=True)\n",
    "    test_IF.to_csv('test-isof-outliers.csv', index=False)\n",
    "    print('\\n Found %d outlier points' % len(train_IF))\n",
    "    timer(start_time)\n",
    "\n",
    "    start_time = timer(None)\n",
    "    threshold = 2.0\n",
    "    print('\\n Running Random Forest Regressor (10-fold):')\n",
    "    target_pred = cross_val_predict(estimator=RFR, X=train_data.values, y=target, cv=10, n_jobs=-1)\n",
    "    rfr_pred = pd.DataFrame({'ID': train_ids, 'y': target, 'y_pred': target_pred})\n",
    "    rfr_pred.to_csv('prediction-train-oof-10fold-RFR.csv', index=False)\n",
    "    yvalues = np.vstack((target, target_pred)).transpose()\n",
    "    OL_score, OL = is_outlier(yvalues, threshold)\n",
    "    train['outlier_score'] = OL_score\n",
    "    myindex = train['outlier_score'] >= threshold\n",
    "    train_OL = train.loc[myindex]\n",
    "    train_OL.reset_index(drop=True, inplace=True)\n",
    "    train_OL.drop(['isof','outlier_score'], axis=1, inplace=True)\n",
    "    train_OL.to_csv('train-outliers.csv', index=False) #train_OL.to_csv(os.path.join(ASSET_PATH, 'train-outliers_' + str('3-5') + '.csv'), index=False)\n",
    "    timer(start_time)\n",
    "\n",
    "    start_time = timer(None)\n",
    "    train_outliers_tsne = train_data_tsne.loc[myindex]\n",
    "    test_outliers_tsne = test_data_tsne.values\n",
    "    outlier_list = []\n",
    "    for k in range(len(train_outliers_tsne)):\n",
    "        d = ((test_outliers_tsne-train_outliers_tsne.values[k])**2).sum(axis=1)  # compute distances\n",
    "        ndx = d.argsort() # sort so that smallest distance is first\n",
    "        print(' Presumed outlier point for train ID = %d is test ID = %d ; their Euclidean distance from t-SNE embedding is %.8f' % (train_OL.iloc[k]['ID'], test.iloc[ndx[0]]['ID'], d[ndx[0]]))\n",
    "        outlier_list.append(ndx[0])\n",
    "        print(' Ten closest test points (ID, distance):')\n",
    "        pprint.pprint(zip(test.iloc[ndx[:10]]['ID'], d[ndx[:10]]))\n",
    "\n",
    "    test_OL = test.iloc[outlier_list]\n",
    "    test_OL.drop(['isof'], axis=1, inplace=True)\n",
    "    test_OL.sort_values(['ID'], inplace=True)\n",
    "    test_OL.reset_index(drop=True, inplace=True)\n",
    "    test_OL.to_csv('test-outliers.csv', index=False)\n",
    "\n",
    "    timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
